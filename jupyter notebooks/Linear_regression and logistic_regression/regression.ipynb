{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Victor J. Maci√° Medina.\n",
    "\n",
    "These are some, improved, notes from a course on data analytics which took place at Washington University in St. Louis. Even if this is based on the classnotes, the final result is extended by introducing explanations of several of the concepts appearing here. This is written from a mathematical perspective but also contains Python implementations of these models.\n",
    "\n",
    "The purpose of these notes is having a consultation guide where going back when needed.\n",
    "\n",
    "## Recalling logistic regression\n",
    "\n",
    "\n",
    "__Likelihood function__\n",
    "\n",
    "$$log(L) = \\sum_{i=1}^{n}y_i log(p_i)+(1-y_i log(1-p_i))$$\n",
    "\n",
    "__Parameter model__ \n",
    "\n",
    "$$p_i = \\frac{1}{1+\\exp(-(\\beta_0+\\beta_1 x_1^i+ \\dots + \\beta_m x_m^i))}$$\n",
    "\n",
    "Here we combine the sigmoid with the linear model.The problem is that the linear model is not giving back probabilities, to solve this problem we use a function taking values between 0 and 1. This function is\n",
    "\n",
    "$$f(x) = \\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "Let's plot this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fcc80552490>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.random.normal(0,1,500)\n",
    "x = 10*x\n",
    "\n",
    "def f(x): #calculates of the sigmoid for a given vector x\n",
    "    y = len(x)*[0]\n",
    "    for i in range(0,len(x)):\n",
    "            y[i] = 1/(1+math.exp(-x[i]))\n",
    "    return y\n",
    "\n",
    "y = f(x)\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The general idea of this method.\n",
    "\n",
    "We have an array of values $y$ which are 0 or 1, yes or no, or in general two categories (We can generalize this to $n$ categories using the multinomial distribution). We want to model this using regression. The first problem we have is that a linear model $\\beta_0 + \\beta_1 x_1^{i}+ \\dots \\beta_m x_m^{i}$ does not work. For instance if you plot GPA against admission (in Graduate School), GPA is a continous variable taking values between 0 and 4 and admission is a discrete variable taking values 1 or 0 (or yes or no).\n",
    "\n",
    "Let's study this in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fcc487408e0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = np.random.randint(2, size=1000)\n",
    "x = 4*np.random.rand(1000)\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This cannot be modelled using linear regression!__\n",
    "\n",
    "One of the variables takes continuous values but the other take discrete values, in particular 0 and 1.\n",
    "\n",
    "Instead of assuming that $y$ follows a linear model we assume that $\\log(\\frac{p_i}{1-p_i})$, where $p_i$ is the parameter of the variable $Y_i$, follows a linear model.\n",
    "\n",
    "In our particular case we have a vector, or an array given by an array of variables, which is a Bernoulli vector $Y$.\n",
    "\n",
    "The log-odds of a Bernoulli variable of parameter $p$ is $\\log(\\frac{p}{1-p})$. We divide the probability of success by the probability of failure and then we take the logarithm of this quantity.\n",
    "\n",
    "Suppose that the log-odd function for the event $Y_i = 1$, follows a linear model. That is, for each $p_i$ we have\n",
    "\n",
    "$$\\log(\\frac{p_i}{1-p_i}) = \\sum_{j=1}^{n}\\beta_j x_j^{i}$$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$\\frac{p_i}{1-p_i} = \\exp\\left(\\sum_{j=1}^{n}\\beta_j x_j^{i}\\right)$$\n",
    "\n",
    "In particular\n",
    "\n",
    "$$p_i = \\frac{1}{1+\\exp\\left(-\\sum_{j=1}^{n}\\beta_j x_j^{i}\\right)}$$\n",
    "\n",
    "\n",
    "Notice that we can use any base for the logarithm. Here I use the natural logarithm. Let's see a complete example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = 2*np.random.rand(5000,1)\n",
    "X2 = 5*np.random.rand(5000,1)\n",
    "X3 = np.random.rand(5000,1)\n",
    "\n",
    "eta = 0.5*X1+0.1*X2+1.56*X3-1\n",
    "X = np.column_stack([X1,X2,X3])\n",
    "p = 1/(1+np.exp(-eta))\n",
    "\n",
    "y = np.random.binomial(1,p).reshape(5000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X[:,1],y)\n",
    "plt.title('Scatter plot')\n",
    "plt.xlabel('X2')\n",
    "plt.ylabel('Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing logistic regression using Newton-Raphson\n",
    "\n",
    "__Hessian matrix__\n",
    "\n",
    "\\begin{align*}\n",
    "H = -X^TWX, \n",
    "\\end{align*}\n",
    "\n",
    "Here $W$ is \n",
    "\n",
    "\\begin{align*}\n",
    "W = \\begin{bmatrix}\n",
    "p_1(1-p_1) & \\dots  & & &  \\\\\n",
    "  & p_2(1-p_2) & & &\\\\\n",
    "\\vdots &  &  &  & \\\\\n",
    " & & & & \\\\\n",
    " & & \\ddots & &  \\\\\n",
    " & & & p_n(1-p_n) &\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "where $W$ is a diagonal matrix.\n",
    "\n",
    "__Newton Raphson algorithm formula__\n",
    "\n",
    "$$\\beta^{(k+1)} = \\beta^{(k)}+(X^TWX)^{-1}X^T(y-p)$$\n",
    "\n",
    "where $k$ denotes the $k-th$ iteration. Here we choose $\\beta^{(0)}$ as initial point. (Recall that in the Newton-Raphson method we choose the first point, we calculate the tangent to the function at that point and then we find a new point close to the zero which is the zero of the tangent line at the initial point. We iterate.)\n",
    "\n",
    "__The idea under this formula__\n",
    "\n",
    "We want to find the parameters $\\beta$. The function we get when differenting the maximum likelihood function is difficult to manage (we want to apply the maximum likelihood estimation).\n",
    "\n",
    "We apply the maximum likelihood estimation when modeling $p_i$ as\n",
    "\n",
    "$$p_i = \\frac{1}{1+\\exp\\left(-\\sum_{j=1}^{m}\\beta_j x_j^{i}\\right)}$$\n",
    "\n",
    "We have \n",
    "\n",
    "\\begin{align*}\n",
    "L(p;\\beta) = \\prod_{j=1}^{m} (1-p_j)^{1-y_j}p_j^{y_j}\n",
    "\\end{align*}\n",
    "\n",
    "We maximize this function respect to $\\beta$ (we assume that $y_i$ is given). We take logarithms \n",
    "\n",
    "\\begin{align*}\n",
    "l = \\log(L) = \\sum_{j=1}^{m}\\left((1-y_j)\\log(1-p_j)+y_j\\log(p_j)\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Taking partial derivatives\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial \\beta_k} = \\sum_{j=1}^{m} \\left( -(1-y_j)\\frac{\\frac{\\partial p_j}{\\partial \\beta_k}}{1-p_j}+y_j\\frac{\\frac{\\partial p_j}{\\partial \\beta_k}}{p_j}\\right) $$\n",
    "\n",
    "Notice that\n",
    "\\begin{align*}\n",
    "\\frac{\\partial p_j}{\\partial \\beta_k} = (1-p_j)p_j\\frac{\\partial \\sum_{l=1}^{m}\\beta_j x_l^{j} }{\\partial \\beta_k} = (1-p_j)p_j x_k^j.\n",
    "\\end{align*}\n",
    "\n",
    "Therefore \n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial l}{\\partial \\beta_k} = \\sum_{j=1}^{m}(y_j-p_j)x_k^j.\n",
    "\\end{align*}\n",
    "\n",
    "We want to solve the equation\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{j=1}^{m}(y_j-p_j)x_k^j = 0\n",
    "\\end{align*}\n",
    "\n",
    "The term $p_j$ is non-linear, in the linear regression model we find $(y_j-x^{i}\\cdot \\beta)$, where $x^{i}$ and $\\beta$ are vectors, in this case the model is linear and we can solve the equation using linear algebra.\n",
    "\n",
    "For the case $\\sum_{j=1}^{m}(p_j-y_j)x_k^j = 0$, where $p_i$ is non linear, we will use an iterative method, the Newton-Raphson algorithm. \n",
    "\n",
    "## Newton-Raphson\n",
    "\n",
    "We will introduce here the Newton-Raphson method in a informal way.\n",
    "Suppose we have real differentiable function, of one real variable, $f(x)$, chose $x_0 \\in \\mathbb{R}$. Suppose that the derivative of $f$ is non zero in a, big enough, open set around the zero we are trying to find (This is a silly hypothesis in practice, we are applying a practical algorithm we are not really worried about the mathematical details). We write the tangent line to $f$ at this point.\n",
    "\n",
    "\\begin{align*}\n",
    "f'(x_0)(x-x_0)+f(x_0)\n",
    "\\end{align*}\n",
    "\n",
    "we find the zero of this line\n",
    "\n",
    "$$x = x_0-\\frac{f(x_0)}{f'(x_0)}$$\n",
    "\n",
    "In general we find an algorithm to find a zero of $f$,\n",
    "\n",
    "$$x_n = x_{n-1}-\\frac{f(x_{n-1})}{f'(x_{n-1})}$$\n",
    "\n",
    "When this algorithm converges, the convergence is cuadratic.\n",
    "\n",
    "We want to apply this algorithm to our previous equation. We can apply this algorithm to the derivative with respect to $\\beta$ of the log-likelihood function. We obtain the formula\n",
    "\n",
    "$$\\beta^{(k+1)} = \\beta^{(k)}-H^{-1}(\\beta) \\nabla log(L)(\\beta)$$\n",
    "\n",
    "where $H$ is the Hessian matrix of $l = log(L)(\\beta)$. This is the Newton-Raphson method in higher dimension.\n",
    "\n",
    "In practice we need to calculate the Hessian matrix. Notice that\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial^2 l}{\\partial \\beta_s \\beta_t} = -p_i(1-p_i)x_s^{i} x_t^{i}\n",
    "\\end{align*}\n",
    "\n",
    "Recall that \n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial l}{\\partial \\beta_k} = \\sum_{j=1}^{m}(y_j-p_j)x_k^j,\n",
    "\\end{align*}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial p_j}{\\partial \\beta_k} = (1-p_j)p_j x_k^j.\n",
    "\\end{align*}\n",
    "\n",
    "If we write this using matrices, we find that\n",
    "\n",
    "$$H = -X^t W X.$$\n",
    "\n",
    "Now recall that \n",
    "\n",
    "$$\\nabla l = (\\frac{\\partial l}{\\partial \\beta_0}, \\frac{\\partial l}{\\partial \\beta_1}, \\dots, \\frac{\\partial l}{\\partial \\beta_m})^t$$\n",
    "\n",
    "and \n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial l}{\\partial \\beta_k} = \\sum_{j=1}^{m}(y_j-p_j)x_k^j,\n",
    "\\end{align*}\n",
    "\n",
    "therefore \n",
    "\n",
    "$$\\nabla l = X^t(y-p)$$\n",
    "\n",
    "Going back to Newton-Raphson, we get \n",
    "\n",
    "$$\\beta^{(k+1)} = \\beta^{(k)}+(X^t W X)^{-1}X^t(y-p)$$\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{align*}\n",
    "W = \\begin{bmatrix}\n",
    "p_1(1-p_1) & \\dots  & & &  \\\\\n",
    "  & p_2(1-p_2) & & &\\\\\n",
    "\\vdots &  &  &  & \\\\\n",
    " & & & & \\\\\n",
    " & & \\ddots & &  \\\\\n",
    " & & & p_n(1-p_n) &\n",
    "\\end{bmatrix}.\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "We initialize this algorithm taking an initial $\\beta_0$. With this $\\beta_0$ we can calculate $p$. We stop the iterations when the error $|\\beta^{k}-\\beta^{(k-1)}|$ is small enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Comments about the implementation__\n",
    "\n",
    "$\\Delta\\beta = (X^t W X)^{-1}X^t(y-p)$ -> deltabeta parameter which varies on each iteration.\n",
    "\n",
    "This is the piece in our previous equation which is giving us the next iteration. Notice that this equation is $\\beta$ dependent, we have to use the 'new beta' in each iteration to calculate $p$ (which depends on beta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.21663222, -0.04182741,  1.02688307]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implementation of the previous algorithm\n",
    "\n",
    "\n",
    "# Here we are using the X values from above, refer to the comment below.\n",
    "\n",
    "X1 = 2*np.random.rand(5000,1)\n",
    "X2 = 5*np.random.rand(5000,1)\n",
    "X3 = np.random.rand(5000,1)\n",
    "\n",
    "eta = 0.5*X1+0.1*X2+1.56*X3-1\n",
    "X = np.column_stack([X1,X2,X3])\n",
    "p = 1/(1+np.exp(-eta))\n",
    "\n",
    "y = np.random.binomial(1,p).reshape(5000,1)\n",
    "\n",
    "\n",
    "# The piece of code above this line was written at the beginning ---\n",
    "# This is just for the sake of clarity.\n",
    "\n",
    "beta = np.array([0.2,0.1,0.1]).reshape(-1,1) # initial beta - transpose - reshape(-1,1)\n",
    "\n",
    "Deltabeta = np.array([0.1,0.1,0.1]).reshape(-1,1) # The initial Deltabeta should be bigger than your while-condition\n",
    "\n",
    "while np.max(np.abs(Deltabeta))>1e-15:\n",
    "       p = 1/(1+np.exp(-np.matmul(X,beta))) # probabilities at each iteration\n",
    "       W = np.diag(np.multiply(p,(1-p))[:,0]) # matrices in the decomposition\n",
    "       XtY = np.matmul(X.T,(y-p))\n",
    "       XtWX = np.matmul(np.matmul(X.T,W),X)\n",
    "       Deltabeta = np.matmul(np.linalg.inv(XtWX),XtY)\n",
    "       beta = beta + Deltabeta\n",
    "beta.T\n",
    "\n",
    "# We can predict the probability of an outcome using this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Really important remark: \n",
    "\n",
    "We are modelling the problem without intercept. When you do this, the results are different than when we model \n",
    "the problem considering the intercept.\n",
    "\n",
    "When implementing Newton-Raphson we have to consider this and adding a column with one at the firt column of the matrix, moving the other columns to the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Model prediction__\n",
    "\n",
    "\n",
    "__Prediction from this logistic-regression model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.55375528])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 1/(1+np.exp(-np.matmul(beta.T,X[0])))\n",
    "p # We are fixing the first column of the matrix X,  \n",
    "  # then this is the predition for p_0 using the logistic regression.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Prediction using the sklearn built-in model__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the sklearn code and its parameters\n",
    "\n",
    "```class sklearn.linear_model.LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.44237929, 0.05100324, 1.4861764 ]]), array([-0.745295])]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty = 'none', solver = 'newton-cg', fit_intercept = False)\n",
    "\n",
    "# We will see the penalty option later, none -> standard logistic regression\n",
    "# solver = 'newton-cg', give you the method to find beta\n",
    "# fit_intercept = False, by default the model provides you beta_0 \n",
    "# if we set false, because we want to compare this with our previous model,\n",
    "# the model does not give you beta_0.\n",
    "\n",
    "lr.fit(X,y.reshape(y.size))\n",
    "\n",
    "print([lr.coef_,lr.intercept_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37867324, 0.62132676]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict_proba(X[0:1]) # This code gives a matrix and not an array X[0:1]\n",
    "\n",
    "# Here we obtain the probability for the first parameter to be zero\n",
    "# and the probability for the second parameter to be one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters we obtain here are the same than our previous parameters.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poisson regression\n",
    "\n",
    "\n",
    "Consider the car accident in St. Louis in a particular day. We have 365 days and the number of accidents per day. Our vector $y$ takes values freely between minus infinity and infinity (in this case, discrete values). \n",
    "\n",
    "We also consider some other variables as weather, etc. In this particular case $y$ could take values, for instance, between 0 and 1000. y will be a function of weather, weakday etc.\n",
    "\n",
    "In the linear regression model we can assume $y$ follows a normal distribution. In the logistic regression we assume Binomial or Bernoulli.\n",
    "In this example we consider the poisson distributions.\n",
    "\n",
    "Recall that the poisson distributions models the number of events happening in a certaing time interval.\n",
    "\n",
    "To clarify a little this. The Poisson distribution gives the probability of a given number of events ocurring in a fixed period of time or space, where the mean rate is constant and each trial is independent.\n",
    "\n",
    "We model our problem using a Poisson distribution. Instead of model $y$ using a linear model we will model $y \\sim Poiss(\\lambda)$ by modeling the parameter $\\lambda$, that is, $\\lambda = f(x) = e^{x \\cdot \\beta}$, where $x,\\beta$ are vectors and $\\cdot$ means dot product.\n",
    "\n",
    "Likelihood function.\n",
    "\n",
    "$$L(y_i,\\lambda_i) = \\frac{\\lambda_i^{y_i}}{y_i!}e^{-\\lambda_i}$$\n",
    "\n",
    "therefore the total likelihood function is\n",
    "\n",
    "\\begin{align*}\n",
    "L = \\prod_{j=1}^{n} \\frac{\\lambda_i^{y_i}}{y_i!}e^{-\\lambda_i}.\n",
    "\\end{align*}\n",
    "\n",
    "Taking logarithms we find that\n",
    "\n",
    "\\begin{align*}\n",
    "l = \\log(L) = \\sum_{i=1}^{n}\\left(y_i\\log(\\lambda_i)-\\lambda_i-log(y_i!)\\right)\n",
    "\\end{align*}\n",
    "\n",
    "Recall that we modeled $\\lambda_i = x^{i} \\cdot \\beta$, where $x^{i}$ is the i-th column of the matrix $X$. Using this remark we find that\n",
    "\n",
    "\\begin{align*}\n",
    "l = \\log(L) = \\sum_{i=1}^{n}\\left(y_i x^{i}\\cdot \\beta-e^{x^{i} \\cdot \\beta}-log(y_i!) \\right).\n",
    "\\end{align*}\n",
    "\n",
    "Taking partial derivatives, we find that\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial l}{\\partial \\beta_k} = \\sum_{i=1}^{n} \\left( y_i x^{i}_k-e^{x^{i} \\cdot \\beta}x^{i}_k \\right) = \\sum_{i=1}^{n} \\left((y_i-e^{x^{i} \\cdot \\beta})x^{i}_k\\right) = \\sum_{i=1}^{n} \\left((y_i-\\lambda_i)x^{i}_k\\right). \n",
    "\\end{align*}\n",
    "\n",
    "We want to maximize $l$. in order to do that we should find the zeros of the previous partial derivative.\n",
    "\n",
    "\n",
    "Again __Newton-Raphson__:\n",
    "\n",
    "\n",
    "Again we have to calculat the Hessian matrix $H$, that is, we have to calculate all the second order partial derivatives. Notice that\n",
    "\n",
    "$$\\frac{\\partial^2 l}{\\partial \\beta_k \\partial \\beta_t} = -\\sum_{i=1}^{n}\\lambda_i x^{i}_k x^{i}_t.$$\n",
    "\n",
    "Therefore we have\n",
    "\\begin{align*}\n",
    "H = X^t \\Lambda X\n",
    "\\end{align*}\n",
    "where $\\Lambda = (\\lambda_i)_{ii}$ is a diagonal matrix.\n",
    "\n",
    "We can apply againg Newton-Raphson to find that\n",
    "\n",
    "$$\\beta^{(k+1)} = \\beta^{(k)}+H \\nabla l$$\n",
    "\n",
    "where $\\beta^{0}$ is the initializing parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Linear Model (GLM)\n",
    "\n",
    "There is a general model containing the previous ones. This is called generalized linear model. \n",
    "\n",
    "In the previous cases we assumed that $y$ was following a normal distribution in the linear regression case, a Bernoulli distribution in the Logistic regression case, and a Poisson distribution in the Poisson regression case.\n",
    "\n",
    "1. In the linear regression case we modeled $\\mu = \\beta \\cdot x$.\n",
    "\n",
    "2. In the logistic regression case we modeled $p = \\frac{1}{1+\\exp(- \\beta \\cdot x)}$.\n",
    "\n",
    "3. In the Poisson regression case we modeled $\\lambda = e^{\\beta \\cdot x}$.\n",
    "\n",
    "\n",
    "Here $\\beta$ and $x$ represent vectors.  In all of these cases we consider the likelihood function. In general we write\n",
    "\n",
    "\\begin{align*}\n",
    "L(y,x; \\beta) = \\prod_{i=1}^{n}L_i(y_i, \\beta \\cdot x^{i}).\n",
    "\\end{align*}\n",
    "\n",
    "Then we use the log-likelihood function $l = log(L)$. We maximize this function using partial derivatives (here we use the maximum likelihood estimate). Then we use Newton-Raphson to find zeros of these partial derivatives. \n",
    "\n",
    "We can extract a general recipe from this procedure. We write the general exponential family\n",
    "\n",
    "\\begin{align*}\n",
    "f(y) = \\exp\\left(\\frac{y\\theta-b(\\theta)}{a(\\phi)}+c(y,\\phi) \\right)\n",
    "\\end{align*}\n",
    "\n",
    "where $a(\\phi)$, $b(\\theta)$ and $c(y,\\phi)$ are functions. If we choose these functions adequately we recover the gaussian, the Bernoulli and the Poisson.\n",
    "\n",
    "1. If we take $a(\\phi) = \\phi^2$, $b(\\theta) =\\frac{1}{2}\\theta^2$ and $c(y,\\phi) = -\\frac{y^2}{2\\phi^2}-\\frac{1}{2}\\log(2\\pi \\phi^2)$ we find that\n",
    "\n",
    "$$f(y) = \\frac{1}{\\sqrt{2\\pi}\\phi}\\exp(-\\frac{(y-\\theta)^2}{2\\phi^2})$$\n",
    "\n",
    "for $\\theta = \\mu$ and $\\phi = \\sigma$ we recover the Normal distribution.\n",
    "\n",
    "2. If we take $\\theta = log(p)-log(1-p)$, $a(\\phi) = 1$, $b(\\theta) = \\log(1+e^{\\theta})$, $c(y,\\phi) = 0$, then we find that\n",
    "\n",
    "$$f(y) = \\exp(y\\theta)\\frac{1}{1+\\exp{\\theta}} = p^{y}(1-p)^{1-y}$$\n",
    "\n",
    "where $e^{\\theta} = \\frac{p}{1-p}$, then $1+e^{\\theta}= 1-p$. Then we recover the Bernoulli distribution.\n",
    "\n",
    "3. This cover a lot of distributions. Look at the link: https://en.wikipedia.org/wiki/Exponential_family#Table_of_distributions\n",
    "\n",
    "## Moments of the exponential family.\n",
    "\n",
    "Let's studyt the moments of the exponential family. First we study the expectation. Let be $Y$ a random variable with law a exponential family. We write\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbb{E}(Y) = \\int_{-\\infty}^{\\infty} yf(y)dy = \\int_{-\\infty}^{\\infty}y \\exp\\left(\\frac{y\\theta-b(\\theta)}{a(\\phi)}+c(y,\\phi)\\right)dy.\n",
    "\\end{align*}\n",
    "\n",
    "We differentiate with respect to $\\theta$ under the integral sign, we do this informally, to find that \n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial \\theta}\\mathbb{E}(Y) = 0 = \\int_{-\\infty}^{\\infty} \\frac{\\partial}{\\partial \\theta} y \\exp\\left(\\frac{y\\theta-b(\\theta)}{a(\\phi)}+c(y,\\phi)\\right)dy\n",
    "\\end{align*}\n",
    "\n",
    "We get that\n",
    "\\begin{align*}\n",
    "E(Y) = b'(\\theta).\n",
    "\\end{align*}\n",
    "\n",
    "This is really informal and __need to be improved__.\n",
    "\n",
    "Some examples:\n",
    "\n",
    "1. Normal distribution, $b(\\theta) = \\frac{1}{2}\\theta^2$, therefore $b'(\\theta) = \\theta$.\n",
    "\n",
    "2. Bernoulli distribution, $b(\\theta) = \\log(1+e^{\\theta})$ and $b'(\\theta) = \\frac{e^{\\theta}}{1+e^{\\theta}} = \\frac{\\frac{p}{1-p}}{1+\\frac{p}{1-p}} = p$.\n",
    "\n",
    "For the variance we have $Var(Y) = a(\\phi)b''(\\theta)$.\n",
    "\n",
    "## Likelihood function of the exponential family.\n",
    "\n",
    "Suppose we have a variable $Y$, which is a vector, and a matrix $X$ conformed by the predictors. We write a matrix $[y^t, X]$.\n",
    "\n",
    "$Y$ could follow any of the distributions given by the exponential family.\n",
    "\n",
    "The Likelihood function for one value is \n",
    "$$L(y_i) = \\exp(\\frac{y_i \\theta-b(\\theta)}{a(\\phi)}+c(y,\\phi))$$\n",
    "\n",
    "Now we made an assumption $E(Y) = b'(\\theta) = g(x \\cdot \\beta)$, where $x$ and $\\beta$ are vectors.\n",
    "\n",
    "1. For instance for the Gaussian distribution, linear regression, we should take $g(z) = z$. \n",
    "\n",
    "2. For logistic regression we should take $g(z) = \\frac{1}{1+\\exp(x \\cdot \\beta)}$\n",
    "\n",
    "The inverse of the function $g$ is called __link function__. In case where $g(z)=b'(z)$, the inverse of this function is call __the canonical link__.\n",
    "\n",
    "## Optimizing the Generalized Linear Model (GLM).\n",
    "\n",
    "We want to find $\\beta$ maximizing the likelihood. The likelihood function is\n",
    "\\begin{align*}\n",
    "L = \\prod_{i=1}^{n} \\exp(\\frac{y_i \\theta -b(\\theta)}{a(\\phi)}+c(y,\\phi))\n",
    "\\end{align*}\n",
    "here $b'(\\theta_i) = g(x^{i} \\cdot \\beta)$.\n",
    "\n",
    "The log-likelihood function is \n",
    "\\begin{align*}\n",
    "l = \\log(L) = \\sum_{i=1}^{n} \\left[ \\frac{y_i \\theta -b(\\theta)}{a(\\phi)}+c(y,\\phi) \\right].\n",
    "\\end{align*}\n",
    "\n",
    "We calculate the parcial derivatives\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial l}{\\partial \\beta_j} = 0\n",
    "\\end{align*}\n",
    "then we get the equality\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^{n}(y_i-b'(\\theta_i))x^{i}_j = 0.\n",
    "\\end{align*}\n",
    "This is the canonical link function.\n",
    "\n",
    "This is telling us that:\n",
    "\n",
    "1. If we take GLM gaussian + canonical link function $\\iff$ linear regression\n",
    "2. If we take GLM Binomial + canonical link funtion $\\iff$ logistic regression\n",
    "3. If we take GLM Poisson + canonical link function $\\iff$ Poisson regression.\n",
    "\n",
    "To use __Newton-Raphson__ we need to calculate the Hessian. This is a complex calculation, I won't write this here. In this calculations everything depends on the data sets.\n",
    "\n",
    "We should use, again, the formula\n",
    "\n",
    "\\begin{align*}\n",
    "\\beta^{k+1} = \\beta^{k}-H^{-1}\\nabla l(\\beta)\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing GLM in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is implemented using a python package\n",
    "\n",
    "# Libraries\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm # This is the library we will use to implement GLM\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Linear Model: Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.42429618, -1.31878899, -1.08886687, ...,  0.9096764 ,\n",
       "        0.14669065, -0.05173123])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.randn(5000,3)\n",
    "Y = .3*X[:,[0]]+X[:,[1]]+2*X[:,[2]]+2+np.random.randn(5000,1)\n",
    "\n",
    "# If we add a bracket inside the matrix we get the transpose directly\n",
    "# I mean the transpose of the vector we are recovering from the matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build GLM in statsmodels\n",
    "\n",
    "__Understading a p-value__\n",
    "\n",
    "- A p-value less than 0.05 is statistically significant.\n",
    "- A p-value higher than 0.05 is statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pd = pd.DataFrame(X) # We need to use dataframes with this library.\n",
    "X_pd = sm.add_constant(X_pd) # create the matrix that will lead to the beta_0 coeff.\n",
    "Y_pd = pd.DataFrame(Y)\n",
    "\n",
    "# To get an intercept we need to have an x_0 variable which is 1. This\n",
    "# is what we do in add constant.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                      0   No. Observations:                 5000\n",
      "Model:                            GLM   Df Residuals:                     4996\n",
      "Model Family:                Gaussian   Df Model:                            3\n",
      "Link Function:               identity   Scale:                         0.97248\n",
      "Method:                          IRLS   Log-Likelihood:                -7022.9\n",
      "Date:                Sat, 10 Oct 2020   Deviance:                       4858.5\n",
      "Time:                        00:17:02   Pearson chi2:                 4.86e+03\n",
      "No. Iterations:                     3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          2.0013      0.014    143.487      0.000       1.974       2.029\n",
      "0              0.2905      0.014     20.915      0.000       0.263       0.318\n",
      "1              0.9954      0.014     72.026      0.000       0.968       1.022\n",
      "2              2.0304      0.014    144.354      0.000       2.003       2.058\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create a Gaussian model with the default link function \n",
    "\n",
    "gaussian_model = sm.GLM(Y_pd,X_pd, family = sm.families.Gaussian())\n",
    "gaussian_model_results = gaussian_model.fit() # take parameters and fit the model\n",
    "print(gaussian_model_results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get back a lot of information. We even recover some confidence intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with Linear Regression in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.29046699, 0.99536256, 2.03043557]]), array([2.00127072])]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(X,Y)\n",
    "print([reg.coef_,reg.intercept_])\n",
    "\n",
    "# We get the same values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Linear Model: Bernoulli distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = 2*np.random.rand(5000,1)\n",
    "X2 = 5*np.random.rand(5000,1)\n",
    "X3 = np.random.rand(5000,1)\n",
    "\n",
    "eta = 0.5*X1+0.1*X2+1.56*X3-1\n",
    "X = np.column_stack([X1,X2,X3])\n",
    "p = 1/(1+np.exp(-eta))\n",
    "\n",
    "y = np.random.binomial(1,p).reshape(5000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframes\n",
    "\n",
    "X_pd = pd.DataFrame(X)\n",
    "X_pd = sm.add_constant(X_pd) # Create the [X,1] matrix\n",
    "Y_pd = pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build GLM statsmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                      0   No. Observations:                 5000\n",
      "Model:                            GLM   Df Residuals:                     4996\n",
      "Model Family:                Binomial   Df Model:                            3\n",
      "Link Function:                  logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -3179.3\n",
      "Date:                Sat, 10 Oct 2020   Deviance:                       6358.6\n",
      "Time:                        00:28:00   Pearson chi2:                 5.00e+03\n",
      "No. Iterations:                     4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.9129      0.094     -9.661      0.000      -1.098      -0.728\n",
      "0              0.4146      0.053      7.886      0.000       0.312       0.518\n",
      "1              0.1066      0.021      5.128      0.000       0.066       0.147\n",
      "2              1.4948      0.106     14.092      0.000       1.287       1.703\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Gaussian family model with the default link function\n",
    "\n",
    "binomial_model = sm.GLM(Y_pd,X_pd, family = sm.families.Binomial())\n",
    "binomial_model_results = binomial_model.fit()\n",
    "\n",
    "print(binomial_model_results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with the logistic model using sklearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.4145558 , 0.10655078, 1.49479068]]), array([-0.91292916])]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(penalty = 'none', solver = 'newton-cg')\n",
    "lr.fit(X,y.reshape(y.size))\n",
    "\n",
    "print([lr.coef_,lr.intercept_])\n",
    "\n",
    "# The result is the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saturated model and deviance.\n",
    "\n",
    "In reality not all the data in the data set is useful. We won't use all the data. Sometimes we will create some variables (when we calculate the intercept we add a one column to $X$).\n",
    "\n",
    "When the length of your data increase, you will have that the likelihood function will increase.\n",
    "\n",
    "A saturated model is a model where there are as many estimated parameters as data points. In a saturated model we must have $\\hat{y}_i = \\hat{\\theta}_i = y_i$. We have been using the hat notation without writing the hat previous to this comment.\n",
    "\n",
    "Let's see an example using the logistic regression. $p_i = y_i$. The saturated model will have log-likelihood function\n",
    "\n",
    "\\begin{align*}\n",
    "l_s = \\sum_{i=1}^{n} y_i \\log(y_i)+(1-y_i)\\log(1-y_i).\n",
    "\\end{align*}\n",
    "\n",
    "Suppose we want to study how far if our model from a saturated model. Suppose that our model has log-likelihood function $l$, then we take the difference\n",
    "$$-2(l-l_s)$$\n",
    "this is called the __Deviance__.\n",
    "\n",
    "Some examples:\n",
    "\n",
    "1. For the Normal distribution the deviance is \n",
    "\\begin{align*}\n",
    "-\\frac{1}{2\\phi}\\sum_{i=1}^{n}(y_i-x^{i}\\cdot \\beta)\n",
    "\\end{align*}\n",
    "where $\\theta_i = x^{i}\\beta$. Here the deviance is almost the MSE (Mean Square Error).\n",
    "\n",
    "Deviance is always bigger than zero. The saturated model has a bigger likelihood."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
